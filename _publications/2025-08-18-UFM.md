---
title: "Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization[in submission]"
collection: publications
permalink: /publication/2025-8-18-UFM
excerpt: 'Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow . It is easier to train and more accurate for large flows compared to the typical coarse-to-find cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.'
date: 2025-08-18
venue: 'IROS 2022'
paperurl: 'https://uniflowmatch.github.io/'
citation: 'Zhang, Yuchen, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade et al. "UFM: A Simple Path towards Unified Dense Correspondence with Flow." arXiv preprint arXiv:2506.09278 (2025).'
---

Author: Zhang, Yuchen, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer & Wenshan Wang (2025)


UFM employs a simple end-to-end transformer architecture. It first encode both images with DINOv2, and then process the concatenated features with self-attention layers. The model then regresses the (u,v) flow image and covisibility prediction through DPT heads. We trained the model on a combined dataset from 12 optical flow and wide-baseline matching datasets, showing mutual improvement on both tasks.

Please refer to the website for further information: https://uniflowmatch.github.io/.